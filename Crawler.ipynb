{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "import psycopg2\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "def update_site_locking(domain, sitemap, robotstxt):\n",
    "    conn = psycopg2.connect(host=\"http://83.212.127.54/\", user=\"test\", password=\"fricrawl\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"INSERT INTO site (domain, robots_content,sitemap_content) VALUES (%s, %s, %s) RETURNING id;\",\n",
    "        (domain, sitemap, robotstxt))\n",
    "        id = cur.fetchone()[0]\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "                    \n",
    "    return id;\n",
    "                    \n",
    "def update_page_locking(siteId, url,html_content, status_code, acc_time, page_type_code):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"INSERT INTO page (site_id, url,html_content,http_status_code,accessed_time, page_type_code) VALUES (%s,%s,%s,%s,%s,%s) RETURNING id;\",\n",
    "        (siteId, url, html_content,status_code,acc_time,page_type_code))\n",
    "        id = cur.fetchone()[0]\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "                    \n",
    "    return id;\n",
    "                    \n",
    "def update_image_locking(pageId, filename, content_type, data, acc_time):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"INSERT INTO image (page_id,filename, content_type,data,accessed_time) VALUES (%s,%s,%s, %s, %s) RETURNING id;\",\n",
    "        (pageId, filename, content_type,data,acc_time))\n",
    "        id = cur.fetchone()[0]\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "                    \n",
    "    return id;\n",
    "                    \n",
    "def update_page_data_locking(pageId, data_type_code, data):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"INSERT INTO page_data (page_id,data_type_code, data) VALUES (%s, %s, %s) RETURNING id;\",\n",
    "        (pageId, data_type_code, data))\n",
    "        id = cur.fetchone()[0]\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "                    \n",
    "    return id;\n",
    "                    \n",
    "                                        \n",
    "def update_link_locking(from_page, to_page):\n",
    "    conn = psycopg2.connect(host=\"localhost\", user=\"user\", password=\"SecretPassword\")\n",
    "    conn.autocommit = True\n",
    "    \n",
    "    with lock:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"INSERT INTO link (from_page,to_page) VALUES (%s, %s);\",\n",
    "        (from_page, to_page))\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43993a62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gov.si\n",
      "tu\n",
      "done1\n",
      "User-agent: *\n",
      "Disallow: /admin\n",
      "Disallow: /resources\n",
      "Disallow: /pomoc\n",
      "\n",
      "Sitemap: https://www.gov.si/sitemap.xml\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error unknown url type: user-agent>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15324/2632778947.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mupdate_site_locking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrobotstxt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msitemap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m \u001b[0mcrawl_site\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15324/2632778947.py\u001b[0m in \u001b[0;36mcrawl_site\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0msitemap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sitemaps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msitemap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mupdate_site_locking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrobotstxt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msitemap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15324/3460265369.py\u001b[0m in \u001b[0;36mget_sitemaps\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mxml\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mXML\u001b[0m \u001b[0msource\u001b[0m \u001b[0mof\u001b[0m \u001b[0mscraped\u001b[0m \u001b[0msitemap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     xml = BeautifulSoup(response, \n\u001b[0;32m     16\u001b[0m                          \u001b[1;34m'lxml-xml'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'urllib.Request'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         return self._call_chain(self.handle_open, 'unknown',\n\u001b[0m\u001b[0;32m    548\u001b[0m                                 'unknown_open', req)\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36munknown_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1423\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0munknown_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m         \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1425\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unknown url type: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse_keqv_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error unknown url type: user-agent>"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import queue\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "frontier = queue.Queue()\n",
    "WEB_DRIVER_LOCATION = \"C:/Users/Pirk/Desktop/faks-mag/ekstrakcija/chromedriver.exe\"\n",
    "TIMEOUT = 5\n",
    "url =  \"https://www.gov.si/\"\n",
    "\n",
    "def is_absolute(link):\n",
    "    return bool(urlparse(link).netloc)\n",
    "\n",
    "def is_link(link):\n",
    "    if (len(re.findall('.:\\/\\/.',link))>0):\n",
    "        return True\n",
    "    return False\n",
    "    #logika - preveri če je stvar valid link ki ga damo v frontier\n",
    "    \n",
    "def make_absolute(baselink,link):\n",
    "    if (True != is_absolute(link)):\n",
    "        return(urljoin(baselink,link))\n",
    "    elif(link[0] == '/' and link[1] == '/'):\n",
    "        return 'https:' + link\n",
    "    return link\n",
    "    #logika - prepozna če je link relativen inga spremeni v absolutnega\n",
    "\n",
    "def get_robotstxt(link):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"user-agent=fri-ieps-TEST\")\n",
    "    driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "    driver.get(link + '/robots.txt')\n",
    "    time.sleep(TIMEOUT)\n",
    "    html = driver.page_source\n",
    "    bsObj = BeautifulSoup(html, 'html.parser')\n",
    "    robots = bsObj.find('pre').contents\n",
    "    return robots\n",
    "\n",
    "def get_sitemap(robotstxt):\n",
    "    robotarray = robotstxt.split()\n",
    "    print(\"dalje:: \\n \"+ robotstxt)\n",
    "    if(\"Sitemap:\" in robotarray):\n",
    "        index = robotarray.index(\"Sitemap:\")\n",
    "        return robotarray[index+1]\n",
    "    return None\n",
    "\n",
    "def getsitemapContext(url):\n",
    "   site = url.find_all(\"sitemap\")\n",
    "    \n",
    "def crawl_site(url):\n",
    "\n",
    "    WEB_PAGE_ADDRESS = url\n",
    "    baseurl = re.match('^.+?[^\\/:](?=[?\\/]|$)', url).group(0)\n",
    "    print(baseurl)\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"user-agent=fri-ieps-TEST\")\n",
    "    driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "    driver.get(WEB_PAGE_ADDRESS)\n",
    "    time.sleep(TIMEOUT)\n",
    "\n",
    "    html = driver.page_source\n",
    "\n",
    "    bsObj = BeautifulSoup(html, 'html.parser')\n",
    "    links = bsObj.find_all('a', href=True)\n",
    "    #finalLinks = set()\n",
    "    print(\"tu\")\n",
    "\n",
    "    for link in links:\n",
    "        if (len(link.attrs['href'])>0 and link.attrs['href'][0] != '#'):\n",
    "            frontier.put(make_absolute(baseurl,link.attrs['href']))\n",
    "            \n",
    "    #print(list(frontier.queue))\n",
    "    print(\"done1\")\n",
    "    domain = baseurl\n",
    "    robotstxt = get_robotstxt(url)\n",
    "\n",
    "    str1 = ''.join(robotstxt)\n",
    "\n",
    "    print(str1)\n",
    "    sitemaplink = get_sitemap(str1)\n",
    "    sitemap = get_sitemaps(sitemaplink)\n",
    "    print(sitemap)\n",
    "    update_site_locking(domain,robotstxt,sitemap)\n",
    "\n",
    "crawl_site(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa8dff",
   "metadata": {},
   "source": [
    "V spodnjem delu kode so metode za pridobivanja sitemapov, potrebno je dobiti .xml naslov iz trenutnega linka - kliči get_sitemap(robots.tx), ki iz robots txt pridobi sitemap.xml, spodnje  funkcije se sprehodijo\n",
    "1\n",
    "\n",
    "Moj naslov: WEB_DRIVER_LOCATION = \"C:/Work/Magisterij_1_leto/2.semester/ekstrakcijaSplet/Nal1/chromedriver.exe\"\n",
    "Juretov naslov: C:/Users/Pirk/Desktop/faks-mag/ekstrakcija/chromedriver.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a29193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import xmltodict\n",
    "\n",
    "def get_sitemaps(url): #find all sitemaps of sitemap from robot.txt\n",
    "    \"\"\"Scrapes an XML sitemap from the provided URL and returns XML source.\n",
    "    Args:\n",
    "        url (string): Fully qualified URL pointing to XML sitemap.\n",
    "    Returns:\n",
    "        xml (string): XML source of scraped sitemap.\n",
    "    \"\"\"\n",
    "    response = urllib.request.urlopen(url)\n",
    "    xml = BeautifulSoup(response, \n",
    "                         'lxml-xml', \n",
    "                         from_encoding=response.info().get_param('charset'))\n",
    "    return xml\n",
    "\n",
    "def get_sitemap_type(xml):\n",
    "    \"\"\"Parse XML source and returns the type of sitemap.\n",
    "    Args:\n",
    "        xml (string): Source code of XML sitemap.\n",
    "    Returns:\n",
    "        sitemap_type (string): Type of sitemap (sitemap, sitemapindex, or None).\n",
    "    \"\"\"\n",
    "    sitemapindex = xml.find_all('sitemapindex')\n",
    "    sitemap = xml.find_all('urlset')\n",
    "    #print(sitemap)\n",
    "    if sitemapindex:\n",
    "        return 'sitemapindex' #vsebujejo linke na otroke\n",
    "    elif sitemap:\n",
    "        return 'urlset' #direktni linki\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def get_child_sitemaps(xml):\n",
    "    \"\"\"Return a list of child sitemaps present in a XML sitemap file.\n",
    "    Args:\n",
    "        xml (string): XML source of sitemap. \n",
    "    Returns:\n",
    "        sitemaps (list): Python list of XML sitemap URLs.\n",
    "    \"\"\"\n",
    "    sitemaps = xml.find_all(\"sitemap\")\n",
    "    output = []\n",
    "    for sitemap in sitemaps:\n",
    "        output.append(sitemap.findNext(\"loc\").text)\n",
    "  \n",
    "    return output\n",
    "def sitemap_to_dataframe(xml, name=None, data=None, verbose=False):\n",
    "    \"\"\"Read an XML sitemap into a Pandas dataframe. \n",
    "\n",
    "    Args:\n",
    "        xml (string): XML source of sitemap. \n",
    "        name (optional): Optional name for sitemap parsed.\n",
    "        verbose (boolean, optional): Set to True to monitor progress.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas dataframe of XML sitemap content. \n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns=['loc', 'changefreq', 'priority', 'domain', 'sitemap_name'])\n",
    "\n",
    "    urls = xml.find_all(\"url\")\n",
    "  \n",
    "    for url in urls:\n",
    "\n",
    "        if xml.find(\"loc\"):\n",
    "            loc = url.findNext(\"loc\").text\n",
    "            parsed_uri = urlparse(loc)\n",
    "            domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        else:\n",
    "            loc = ''\n",
    "            domain = ''\n",
    "\n",
    "        if xml.find(\"changefreq\"):\n",
    "            changefreq = url.findNext(\"changefreq\").text\n",
    "        else:\n",
    "            changefreq = ''\n",
    "\n",
    "        if xml.find(\"priority\"):\n",
    "            priority = url.findNext(\"priority\").text\n",
    "        else:\n",
    "            priority = ''\n",
    "\n",
    "        if name:\n",
    "            sitemap_name = name\n",
    "        else:\n",
    "            sitemap_name = ''\n",
    "              \n",
    "        row = {\n",
    "            'domain': domain,\n",
    "            'loc': loc,\n",
    "            'changefreq': changefreq,\n",
    "            'priority': priority,\n",
    "            'sitemap_name': sitemap_name,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(row)\n",
    "\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    return df\n",
    "def get_all_urls_SiteMap(url): #provide xml of a site , from robot.txt ,... \n",
    "    \"\"\"Return a dataframe containing all of the URLs from a site's XML sitemaps.\n",
    "    Args:\n",
    "        url (string): URL of site's XML sitemap. Usually located at /sitemap.xml\n",
    "    Returns:\n",
    "        df (dataframe): Pandas dataframe containing all sitemap content. \n",
    "\n",
    "    \"\"\"  \n",
    "    xml = get_sitemaps(url)\n",
    "    sitemap_type = get_sitemap_type(xml)\n",
    "    if sitemap_type =='sitemapindex':\n",
    "        sitemaps = get_child_sitemaps(xml)\n",
    "    else:\n",
    "        sitemaps = [url]\n",
    "    df = pd.DataFrame(columns=['loc', 'changefreq', 'priority', 'domain', 'sitemap_name'])\n",
    "    for sitemap in sitemaps:\n",
    "        #print(sitemap)\n",
    "        sitemap_xml = get_sitemaps(sitemap) # ce želimo imeti vse povezave sitemapov\n",
    "        #  sitemaps_all.append(sitemap_xml) \n",
    "        df_sitemap = sitemap_to_dataframe(sitemap_xml, name=sitemap)\n",
    "        print(sitemap_xml)\n",
    "        #print(df_sitemap)\n",
    "        #print(\"end\")\n",
    "        df = pd.concat([df, df_sitemap], ignore_index=True)\n",
    "        print(sitemap)\n",
    "        #file = urllib.request.urlopen(sitemap)  #odpri xml in pridobi podatke\n",
    "        #data = file.read()\n",
    "        #file.close()\n",
    "        #data = xmltodict.parse(data)\n",
    "        #print(data)\n",
    "    return df\n",
    "\n",
    "url=\"https://www.gov.si/sitemap.xml\" #dobimo ga recimo z robots.txt\n",
    "dataFrame = get_all_urls_SiteMap(url)\n",
    "#print(dataFrame.head())\n",
    "#print(dataFrame.sitemap_name.value_counts())\n",
    "#xml=get_sitemap(url)\n",
    "#sitemap_type = get_sitemap_type(xml)\n",
    "#if(sitemap_type==\"sitemapindex\"): #če je \n",
    "    #child_sitemaps = get_child_sitemaps(xml)\n",
    "    #print(child_sitemaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65286956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
