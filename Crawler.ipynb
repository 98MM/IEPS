{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26742cc6",
   "metadata": {},
   "source": [
    "Naloga pajka:\n",
    "1. HTTP downloader and renderer: To retrieve and render a web page.\n",
    "2. Data extractor: Minimal functionalities to extract images and hyperlinks.\n",
    "3. Duplicate detector: To detect already parsed pages.\n",
    "4. URL frontier: A list of URLs waiting to be parsed.\n",
    "5. Datastore: To store the data and additional metadata used by the crawler.\n",
    "\n",
    "TO-DO 2-images exctraction, duplicate detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3846fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "import psycopg2\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def reset_db():\n",
    "    \n",
    "    conn = psycopg2.connect(host=\"83.212.127.54\",dbname='crawler', user=\"test\", password=\"fricrawl\")\n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"DELETE FROM crawldb.image\")\n",
    "    cur.execute(\"DELETE FROM crawldb.page_data\")\n",
    "    cur.execute(\"DELETE FROM crawldb.link\")\n",
    "    cur.execute(\"DELETE FROM crawldb.page\")\n",
    "    cur.execute(\"DELETE FROM crawldb.site\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "  \n",
    "\n",
    "def update_site_locking(domain, sitemap, robotstxt):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn = psycopg2.connect(host=\"83.212.127.54\",dbname='crawler', user=\"test\", password=\"fricrawl\")\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"INSERT INTO crawldb.site (domain, robots_content,sitemap_content) VALUES (%s, %s, %s) RETURNING id;\",\n",
    "            (domain, sitemap, robotstxt))\n",
    "            id = -1\n",
    "            if cur.rowcount != 0:\n",
    "                id = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if id != -1:\n",
    "                return id;\n",
    "            else:\n",
    "                print(\"Error with cur in update_site_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_site_locking: \", e)\n",
    "        finally:\n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "                    \n",
    "    \n",
    "                    \n",
    "def update_page_locking(siteId, url,html_content, status_code, acc_time, page_type_code):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn = psycopg2.connect(host=\"83.212.127.54\",dbname='crawler', user=\"test\", password=\"fricrawl\")\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"INSERT INTO crawldb.page (site_id, url,html_content,http_status_code,accessed_time, page_type_code) VALUES (%s,%s,%s,%s,%s,%s) RETURNING id;\",\n",
    "            (siteId, url, html_content,status_code,acc_time,page_type_code))\n",
    "            id = -1\n",
    "            if cur.rowcount != 0:\n",
    "                id = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if id != -1:\n",
    "                return id;\n",
    "            else:\n",
    "                print(\"Error with cur in update_page_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_page_locking: \", e)\n",
    "        finally:\n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "                    \n",
    "    \n",
    "                    \n",
    "def update_image_locking(pageId, filename, content_type, data, acc_time):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn = psycopg2.connect(host=\"83.212.127.54\",dbname='crawler', user=\"test\", password=\"fricrawl\")\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"INSERT INTO crawldb.image (page_id,filename, content_type,data,accessed_time) VALUES (%s,%s,%s, %s, %s) RETURNING id;\",\n",
    "            (pageId, filename, content_type,data,acc_time))\n",
    "            id = -1\n",
    "            if cur.rowcount != 0:\n",
    "                id = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if id != -1:\n",
    "                return id;\n",
    "            else:\n",
    "                print(\"Error with cur in update_image_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_image_locking: \", e)\n",
    "        finally:\n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "                    \n",
    "    \n",
    "                    \n",
    "def update_page_data_locking(pageId, data_type_code, data):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn = psycopg2.connect(host=\"83.212.127.54\",dbname='crawler', user=\"test\", password=\"fricrawl\")\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"INSERT INTO crawldb.page_data (page_id,data_type_code, data) VALUES (%s, %s, %s) RETURNING id;\",\n",
    "            (pageId, data_type_code, data))\n",
    "            id = -1\n",
    "            if cur.rowcount != 0:\n",
    "                id = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if id != -1:\n",
    "                return id;\n",
    "            else:\n",
    "                print(\"Error with cur in update_page_data_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_page_data_locking: \", e)\n",
    "        finally:\n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "                    \n",
    "    \n",
    "                    \n",
    "                                        \n",
    "def update_link_locking(from_page, to_page):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn = psycopg2.connect(host=\"83.212.127.54\",dbname='crawler', user=\"test\", password=\"fricrawl\")\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"INSERT INTO crawldb.link (from_page,to_page) VALUES (%s, %s);\",\n",
    "            (from_page, to_page))\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_link_locking: \", e)\n",
    "        finally:\n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "    \n",
    "def get_domain_id_locking(domain):\n",
    "\n",
    "    with lock:\n",
    "        try:\n",
    "            conn = psycopg2.connect(host=\"83.212.127.54\",dbname='crawler', user=\"test\", password=\"fricrawl\")\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT id FROM crawldb.site WHERE domain = %s\",\n",
    "            (domain,))\n",
    "            id = None\n",
    "            if cur.rowcount != 0:\n",
    "                id = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if id is not None:\n",
    "                return id;\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_link_locking: \", e)\n",
    "        finally:\n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "                \n",
    "                \n",
    "def get_domain_robots_locking(domain):\n",
    "\n",
    "    with lock:\n",
    "        try:\n",
    "            conn = psycopg2.connect(host=\"83.212.127.54\",dbname='crawler', user=\"test\", password=\"fricrawl\")\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT robots_content FROM crawldb.site WHERE id = %s\",\n",
    "            (domain,))\n",
    "            robots = -1\n",
    "            if cur.rowcount != 0:\n",
    "                robots = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if robots != -1:\n",
    "                return robots;\n",
    "            else:\n",
    "                print(\"Error with cur in update_image_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_link_locking: \", e)\n",
    "        finally:\n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "                 \n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37897c11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_absolute' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 93>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m         dissaloved\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dissaloved,allowed\n\u001b[1;32m---> 93\u001b[0m data,sitemaps\u001b[38;5;241m=\u001b[39m\u001b[43mrobots_to_String\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://fortune.com/robots\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#klici to da dobis df\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#print(robotsdf)\u001b[39;00m\n\u001b[0;32m     95\u001b[0m dissallowed,allowed\u001b[38;5;241m=\u001b[39mgetAllow_Dissalow(data)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mrobots_to_String\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 ls[ua]\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[0;32m     75\u001b[0m                 ls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m---> 76\u001b[0m                 ls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPattern\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmake_absolute\u001b[49m(url,value))\n\u001b[0;32m     77\u001b[0m                 \u001b[38;5;66;03m#ls['Pattern'].append(value)\u001b[39;00m\n\u001b[0;32m     78\u001b[0m robots_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(ls)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_absolute' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    " \n",
    "ua = 'User-agent'\n",
    "\n",
    "\n",
    "def get_robots_url(url):\n",
    "    domain_url = '{uri.scheme}://{uri.netloc}'.format(uri=urlparse(url))\n",
    "    robots_url = domain_url + '/robots.txt'\n",
    "    return robots_url\n",
    " \n",
    "def read_robots_txt(url):\n",
    "    robot_url = get_robots_url(url)\n",
    "    robot_file = os.popen(f'curl {robot_url}').read()\n",
    "    return robot_file\n",
    " \n",
    "def initialize_dict(url):\n",
    "    robot_file = read_robots_txt(url)\n",
    "    result_data_set = {ua:{}}\n",
    "    for line in robot_file.split(\"\\n\"):\n",
    "        if line.startswith(ua):\n",
    "            result_data_set[ua].update({line.split(':')[1].strip():{}})\n",
    "    keys = []\n",
    "    for key in result_data_set[ua].keys():\n",
    "        keys.append(key)\n",
    "    return result_data_set, keys, robot_file,make_sitemaps(robot_file)\n",
    "\n",
    "def make_sitemaps(robots):\n",
    "    data = []\n",
    "    lines = str(robots).splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('Sitemap:'):\n",
    "            split = line.split(':', maxsplit=1)\n",
    "            data.append(split[1].strip())\n",
    "\n",
    "    return data\n",
    "def parse_robot(url):\n",
    "    idict = initialize_dict(url)\n",
    "    result_data_set = idict[0]\n",
    "    keys = idict[1]\n",
    "    robot_file = idict[2]\n",
    "    sitemaps=idict[3]\n",
    "    print_flag = False\n",
    "    for i in range(len(keys)):\n",
    "        if i <= len(keys)-2:\n",
    "            end_str = keys[i+1]\n",
    "        else:\n",
    "            end_str = 'We are done'\n",
    " \n",
    "        result_data_set[ua][keys[i]]['Disallow'] = []\n",
    "        result_data_set[ua][keys[i]]['Allow'] = []\n",
    "        for line in robot_file.split(\"\\n\"):\n",
    "            if end_str in line:\n",
    "                print_flag = False\n",
    "            elif keys[i] in line:\n",
    "                print_flag = True\n",
    "            elif print_flag:\n",
    "                if line.startswith('Disallow') or line.startswith('Allow'):\n",
    "                    status = line.split(':')[0].strip()\n",
    "                    val = line.split(':')[1].strip()\n",
    "                    result_data_set[ua][keys[i]][status].append(val)\n",
    "    return result_data_set,sitemaps\n",
    "\n",
    "def robots_to_String(url):\n",
    "    result_data_set,sitemaps = parse_robot(url)\n",
    "    ls = {ua:[],'Status':[],'Pattern':[]}\n",
    "    for k,v in result_data_set.items():\n",
    "        for v in result_data_set[k]:\n",
    "            for key,value in result_data_set[k][v].items():\n",
    "                for value in result_data_set[k][v][key]:\n",
    "                    ls[ua].append(v)\n",
    "                    ls['Status'].append(key)\n",
    "                    ls['Pattern'].append(make_absolute(url,value))\n",
    "                    #ls['Pattern'].append(value)\n",
    "    robots_df = pd.DataFrame.from_dict(ls)\n",
    "    return pd.DataFrame.to_string(robots_df),sitemaps #robots_df\n",
    "\n",
    "def getAllow_Dissalow(data): #rabi pandas DF\n",
    "    data = io.StringIO(data)\n",
    "    df= pd.read_csv(data, sep='\\s+')\n",
    "    if \"User-agent\" in df:\n",
    "        df=df[(df[\"User-agent\"]==\"*\") | (df[\"User-agent\"]==ua)] ##vse ki niso moj uporabnik oz. *#lahko odstranim ker se me ne tičejo\n",
    "        allowed=df[df['Status']==\"Allow\"].Pattern.tolist()\n",
    "        dissaloved=df[df['Status']==\"Disallow\"].Pattern.tolist()\n",
    "    else:\n",
    "        allowed=[]\n",
    "        dissaloved=[]\n",
    "    return dissaloved,allowed\n",
    "\n",
    "data,sitemaps=robots_to_String(\"https://fortune.com/robots\") #klici to da dobis df\n",
    "#print(robotsdf)\n",
    "dissallowed,allowed=getAllow_Dissalow(data)\n",
    "print(dissallowed)\n",
    "print(allowed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c0f50",
   "metadata": {},
   "source": [
    "['https://fortune.com/wp-admin/', 'https://fortune.com/sponsored/', 'https://fortune.com/feeds/', \n",
    " 'https://fortune.com/feed/', 'https://fortune.com/wp-login.php', 'https://fortune.com/wp-signup.php', \n",
    " 'https://fortune.com/press-this.php', 'https://fortune.com/remote-login.php', 'https://fortune.com/activate/',\n",
    " 'https://fortune.com/cgi-bin/', 'https://fortune.com/mshots/v1/', 'https://fortune.com/next/', \n",
    " 'https://fortune.com/sponsored/', 'https://fortune.com/feeds/', 'https://fortune.com/feed/', 'https://fortune.com/wp-login.php', 'https://fortune.com/wp-signup.php', 'https://fortune.com/press-this.php', 'https://fortune.com/remote-login.php', 'https://fortune.com/activate/', 'https://fortune.com/cgi-bin/', 'https://fortune.com/mshots/v1/', 'https://fortune.com/next/']\n",
    "['https://fortune.com/wp-admin/admin-ajax.php']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b688db",
   "metadata": {},
   "source": [
    "V spodnjem delu kode so metode za pridobivanja sitemapov, potrebno je dobiti .xml naslov iz trenutnega linka - kliči get_sitemap(robots.tx), ki iz robots txt pridobi sitemap.xml, spodnje  funkcije se sprehodijo\n",
    "1. poženi urlSitemap=get_site_map(robots.txt)\n",
    "2. get_all_urls_siteMap(urlSitemap)\n",
    "\n",
    "Moj naslov: WEB_DRIVER_LOCATION = \"C:/Work/Magisterij_1_leto/2.semester/ekstrakcijaSplet/Nal1/chromedriver.exe\"\n",
    "Juretov naslov: C:/Users/Pirk/Desktop/faks-mag/ekstrakcija/chromedriver.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a29193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import xmltodict\n",
    "\n",
    "def get_sitemaps(url): #find all sitemaps of sitemap from robot.txt\n",
    "    \"\"\"Scrapes an XML sitemap from the provided URL and returns XML source.\n",
    "    Args:\n",
    "        url (string): Fully qualified URL pointing to XML sitemap.\n",
    "    Returns:\n",
    "        xml (string): XML source of scraped sitemap.\n",
    "    \"\"\"\n",
    "    response = urllib.request.urlopen(url)\n",
    "    xml = BeautifulSoup(response, \n",
    "                         'lxml-xml', \n",
    "                         from_encoding=response.info().get_param('charset'))\n",
    "    return xml\n",
    "\n",
    "def get_sitemap_type(xml):\n",
    "    \"\"\"Parse XML source and returns the type of sitemap.\n",
    "    Args:\n",
    "        xml (string): Source code of XML sitemap.\n",
    "    Returns:\n",
    "        sitemap_type (string): Type of sitemap (sitemap, sitemapindex, or None).\n",
    "    \"\"\"\n",
    "    sitemapindex = xml.find_all('sitemapindex')\n",
    "    sitemap = xml.find_all('urlset')\n",
    "    #print(sitemap)\n",
    "    if sitemapindex:\n",
    "        return 'sitemapindex' #vsebujejo linke na otroke\n",
    "    elif sitemap:\n",
    "        return 'urlset' #direktni linki\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def get_child_sitemaps(xml):\n",
    "    \"\"\"Return a list of child sitemaps present in a XML sitemap file.\n",
    "    Args:\n",
    "        xml (string): XML source of sitemap. \n",
    "    Returns:\n",
    "        sitemaps (list): Python list of XML sitemap URLs.\n",
    "    \"\"\"\n",
    "    sitemaps = xml.find_all(\"sitemap\")\n",
    "    output = []\n",
    "    for sitemap in sitemaps:\n",
    "        output.append(sitemap.findNext(\"loc\").text)\n",
    "  \n",
    "    return output\n",
    "def sitemap_to_dataframe(xml, name=None, data=None, verbose=False):\n",
    "    \"\"\"Read an XML sitemap into a Pandas dataframe. \n",
    "\n",
    "    Args:\n",
    "        xml (string): XML source of sitemap. \n",
    "        name (optional): Optional name for sitemap parsed.\n",
    "        verbose (boolean, optional): Set to True to monitor progress.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas dataframe of XML sitemap content. \n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns=['loc', 'changefreq', 'priority', 'domain', 'sitemap_name'])\n",
    "\n",
    "    urls = xml.find_all(\"url\")\n",
    "  \n",
    "    for url in urls:\n",
    "\n",
    "        if xml.find(\"loc\"):\n",
    "            loc = url.findNext(\"loc\").text\n",
    "            parsed_uri = urlparse(loc)\n",
    "            domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        else:\n",
    "            loc = ''\n",
    "            domain = ''\n",
    "\n",
    "        if xml.find(\"changefreq\"):\n",
    "            changefreq = url.findNext(\"changefreq\").text\n",
    "        else:\n",
    "            changefreq = ''\n",
    "\n",
    "        if xml.find(\"priority\"):\n",
    "            priority = url.findNext(\"priority\").text\n",
    "        else:\n",
    "            priority = ''\n",
    "\n",
    "        if name:\n",
    "            sitemap_name = name\n",
    "        else:\n",
    "            sitemap_name = ''\n",
    "              \n",
    "        row = {\n",
    "            'domain': domain,\n",
    "            'loc': loc,\n",
    "            'changefreq': changefreq,\n",
    "            'priority': priority,\n",
    "            'sitemap_name': sitemap_name,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(row)\n",
    "\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    return df\n",
    "def get_all_urls_SiteMap(url): #provide xml of a site , from robot.txt ,... \n",
    "    \"\"\"Return a dataframe containing all of the URLs from a site's XML sitemaps.\n",
    "    Args:\n",
    "        url (string): URL of site's XML sitemap. Usually located at /sitemap.xml\n",
    "    Returns:\n",
    "        df (dataframe): Pandas dataframe containing all sitemap content. \n",
    "\n",
    "    \"\"\"  \n",
    "    xml = get_sitemaps(url)\n",
    "    sitemap_type = get_sitemap_type(xml)\n",
    "    if sitemap_type =='sitemapindex':\n",
    "        sitemaps = get_child_sitemaps(xml)\n",
    "    else:\n",
    "        sitemaps = [url]\n",
    "    df = pd.DataFrame(columns=['loc', 'changefreq', 'priority', 'domain', 'sitemap_name'])\n",
    "    for sitemap in sitemaps:\n",
    "        #print(sitemap)\n",
    "        sitemap_xml = get_sitemaps(sitemap) # ce želimo imeti vse povezave sitemapov\n",
    "        #  sitemaps_all.append(sitemap_xml) \n",
    "        df_sitemap = sitemap_to_dataframe(sitemap_xml, name=sitemap)\n",
    "        print(sitemap_xml)\n",
    "        #print(df_sitemap)\n",
    "        #print(\"end\")\n",
    "        df = pd.concat([df, df_sitemap], ignore_index=True)\n",
    "        print(sitemap)\n",
    "        #file = urllib.request.urlopen(sitemap)  #odpri xml in pridobi podatke\n",
    "        #data = file.read()\n",
    "        #file.close()\n",
    "        #data = xmltodict.parse(data)\n",
    "        #print(data)\n",
    "    return df\n",
    "\n",
    "url=\"https://www.gov.si/sitemap.xml\" #dobimo ga recimo z robots.txt\n",
    "dataFrame = get_all_urls_SiteMap(url)\n",
    "print(dataFrame.head())\n",
    "print(dataFrame.sitemap_name.value_counts())\n",
    "xml=get_sitemap(url)\n",
    "#sitemap_type = get_sitemap_type(xml)\n",
    "#if(sitemap_type==\"sitemapindex\"): #če je \n",
    "    #child_sitemaps = get_child_sitemaps(xml)\n",
    "    #print(child_sitemaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43993a62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "https://www.gov.si baseurl\n",
      "http://evem.gov.si baseurl\n",
      "https://e-uprava.gov.si baseurl\n",
      "DONE 3\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "https://www.e-prostor.gov.si baseurl\n",
      "http://www.gov.si baseurl\n",
      "http://www.eugo.gov.si baseurl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-65:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1040, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 416, in connect\n",
      "    self.sock = ssl_wrap_socket(\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 449, in ssl_wrap_socket\n",
      "    ssl_sock = _ssl_wrap_socket_impl(\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\ssl.py\", line 500, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\ssl.py\", line 1040, in _create\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\ssl.py\", line 1309, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1125)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 440, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 785, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.podjetniski-portal.si', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1125)')))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Pirk\\AppData\\Local\\Temp\\ipykernel_30632\\1417820828.py\", line 228, in pajek\n",
      "  File \"C:\\Users\\Pirk\\AppData\\Local\\Temp\\ipykernel_30632\\1417820828.py\", line 112, in crawl_site\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\requests\\api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 667, in send\n",
      "    history = [resp for resp in gen]\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 667, in <listcomp>\n",
      "    history = [resp for resp in gen]\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 237, in resolve_redirects\n",
      "    resp = self.send(\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 517, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='www.podjetniski-portal.si', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1125)')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.podjetniski-portal.si baseurl\n",
      "DONE 4\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "None id v bazo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-69:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Pirk\\AppData\\Local\\Temp\\ipykernel_30632\\1417820828.py\", line 215, in pajek\n",
      "  File \"C:\\Users\\Pirk\\AppData\\Local\\Temp\\ipykernel_30632\\3846591455.py\", line 68, in robots_to_String\n",
      "  File \"C:\\Users\\Pirk\\AppData\\Local\\Temp\\ipykernel_30632\\3846591455.py\", line 41, in parse_robot\n",
      "  File \"C:\\Users\\Pirk\\AppData\\Local\\Temp\\ipykernel_30632\\3846591455.py\", line 20, in initialize_dict\n",
      "  File \"C:\\Users\\Pirk\\AppData\\Local\\Temp\\ipykernel_30632\\3846591455.py\", line 16, in read_robots_txt\n",
      "  File \"C:\\Users\\Pirk\\anaconda3\\lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 1438: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.izvoznookno.si baseurl\n",
      "http://www.podjetniskisklad.si baseurl\n",
      "https://evem.gov.si baseurl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import queue\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "from url_normalize import url_normalize\n",
    "import hashlib\n",
    "import datetime\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "#import magic\n",
    "\n",
    "\n",
    "visited_urls = []\n",
    "visited = {}\n",
    "domains = {}\n",
    "frontier = queue.Queue()\n",
    "WEB_DRIVER_LOCATION = \"C:/Users/Pirk/Desktop/faks-mag/ekstrakcija/chromedriver.exe\" #jure\n",
    "#WEB_DRIVER_LOCATION = \"C:/Work/Magisterij_1_leto/2.semester/ekstrakcijaSplet/Nal1/chromedriver.exe\" #matjaž\n",
    "#WEB_DRIVER_LOCATION = \"C:/Users/miham/Documents/Faks/IEPS/chromedriver.exe\" #Miha\n",
    "TIMEOUT = 5\n",
    "#url =  \"https://www.gov.si/\"\n",
    "sha256 = hashlib.sha256()\n",
    "\n",
    "def is_absolute(link):\n",
    "    return bool(urlparse(link).netloc)\n",
    "\n",
    "def is_link(link):\n",
    "    if (len(re.findall('.:\\/\\/.',link))>0):\n",
    "        return True\n",
    "    return False\n",
    "    #logika - preveri če je stvar valid link ki ga damo v frontier\n",
    "    \n",
    "def make_absolute(baselink,link):\n",
    "    \n",
    "    if (True != is_absolute(link)):\n",
    "        return(urljoin(baselink,link))\n",
    "\n",
    "    elif(link[0] == '/' and link[1] == '/'):\n",
    "        return 'http:' + link\n",
    "\n",
    "    return link\n",
    "    #logika - prepozna če je link relativen inga spremeni v absolutnega\n",
    "\n",
    "def get_robotstxt(link):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"user-agent=fri-ieps-TEST\")\n",
    "    driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "    driver.get(link + '/robots.txt')\n",
    "    time.sleep(TIMEOUT)\n",
    "    html = driver.page_source\n",
    "    bsObj = BeautifulSoup(html, 'html.parser')\n",
    "    robots = bsObj.find('pre').contents\n",
    "    return robots\n",
    "\n",
    "def get_sitemap(robotstxt):\n",
    "    robotarray = robotstxt.split()\n",
    "    if(\"Sitemap:\" in robotarray):\n",
    "        index = robotarray.index(\"Sitemap:\")\n",
    "        return robotarray[index+1]\n",
    "    return None\n",
    "\n",
    "def getsitemapContext(url):\n",
    "   site = url.find_all(\"sitemap\")\n",
    "\n",
    "def checkduplicate(html):\n",
    "    #print(hashlib.sha224(str(html).encode(\"utf-8\")).hexdigest())\n",
    "\n",
    "    if (hashlib.sha224(str(html).encode(\"utf-8\")).hexdigest() in visited):\n",
    "        return visited[hashlib.sha224(str(html).encode(\"utf-8\")).hexdigest()]\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def get_ending(url):\n",
    "    stays_same = ['pdf', 'doc', 'docx', 'ppt', 'pptx']\n",
    "    \n",
    "    split_url = url.split('.')\n",
    "    if(len(split_url)>0):\n",
    "        last = split_url[-1]\n",
    "        if last in stays_same:\n",
    "            return last\n",
    "        else:\n",
    "            return 'html'\n",
    "    else:\n",
    "        return 'drop'\n",
    "\n",
    "    \n",
    "def isallowed(url, dissaloved):\n",
    "\n",
    "    for link in dissaloved:\n",
    "        if(link == url):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def crawl_site(url,siteid,dissaloved, allowed):\n",
    "\n",
    "    if url not in visited_urls:\n",
    "        visited_urls.append(url)\n",
    "\n",
    "        WEB_PAGE_ADDRESS = url\n",
    "        baseurl = re.match('^.+?[^\\/:](?=[?\\/]|$)', url).group(0)\n",
    "        print(baseurl,'baseurl')\n",
    "        acc_time = datetime.datetime.fromtimestamp(time.time())\n",
    "\n",
    "        response = requests.get(WEB_PAGE_ADDRESS)\n",
    "        statusCode = response.status_code\n",
    "        time.sleep(TIMEOUT)\n",
    "\n",
    "        \n",
    "        if(statusCode >= 300):\n",
    "            update_page_locking(siteid, url_normalize(url),'', statusCode, acc_time, 'HTML')\n",
    "            return\n",
    "    \n",
    "             \n",
    "        \n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"user-agent=fri-ieps-friendship\")\n",
    "        driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "        try:\n",
    "            driver.get(WEB_PAGE_ADDRESS)\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_link_locking: \", e)\n",
    "            return\n",
    "            \n",
    "\n",
    "\n",
    "        time.sleep(TIMEOUT)\n",
    "        \n",
    "\n",
    "\n",
    "        html = driver.page_source       \n",
    "        isVisited = checkduplicate(html)\n",
    "\n",
    "        if(isVisited != -1):\n",
    "            pageid = update_page_locking(siteid, url_normalize(url),'', statusCode, acc_time, 'DUPLICATE')\n",
    "            update_link_locking(isVisited, pageid)\n",
    "            return\n",
    "        \n",
    "        bsObj = BeautifulSoup(html, 'html.parser')\n",
    "        links = bsObj.find_all('a', href=True)\n",
    "        img_tags = bsObj.findAll('img')\n",
    "\n",
    "        img_urls = [img['src'] for img in img_tags]\n",
    "        img_db_objs = []    \n",
    "        for img_url in img_urls:\n",
    "            split_url = img_url.split('/')\n",
    "            name = split_url[-1]\n",
    "            filetype = name[-4:]\n",
    "            name = name[:-4]\n",
    "            data = img_url\n",
    "            accessed_time = time.time()\n",
    "\n",
    "            img_db = {\n",
    "                'filename': name,\n",
    "                'content_type': filetype,\n",
    "                'data': data,\n",
    "                'accessed_time': accessed_time\n",
    "            }\n",
    "            img_db_objs.append(img_db)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        page_type_code = \"\"\n",
    "        #print(url_normalize(url))\n",
    "        head = bsObj.find('head').contents\n",
    "        if head is None:\n",
    "            page_type_code = \"BINARY\"\n",
    "        else:\n",
    "            page_type_code = \"HTML\"\n",
    "            \n",
    "        \n",
    "        pageid = update_page_locking(siteid, url_normalize(url),str(html), str(statusCode), acc_time, page_type_code)\n",
    "        visited[hashlib.sha224(str(html).encode(\"utf-8\")).hexdigest()] = pageid\n",
    "            \n",
    "            \n",
    "            \n",
    "        for link in links:\n",
    "            if (len(link.attrs['href'])>0 and link.attrs['href'][0] != '#'):\n",
    "                absolute_link = make_absolute(baseurl,link.attrs['href'])\n",
    "                ending = get_ending(absolute_link)\n",
    "                if(ending == 'html'):\n",
    "                    if(isallowed(absolute_link,dissaloved)):\n",
    "                        frontier.put(absolute_link)\n",
    "                elif(ending == 'drop'):\n",
    "                    continue\n",
    "                else: \n",
    "                    #print(\"TU SE NEKE ZAPISE V BAZO\")\n",
    "                    update_page_data_locking(pageid, ending.upper(), '')\n",
    "                    #zapisi v bazo\n",
    "        \n",
    "        for img_db in img_db_objs:\n",
    "            update_image_locking(pageid,img_db['filename'], img_db['content_type'], img_db['data'], datetime.datetime.fromtimestamp(img_db['accessed_time']))\n",
    "        \n",
    "       \n",
    "        \n",
    "def pajek(url):\n",
    "    \n",
    "    #url = frontier.get()\n",
    "    domain = re.match('^.+?[^\\/:](?=[?\\/]|$)', url).group(0)\n",
    "\n",
    "    siteid = '666'\n",
    "    baseid = get_domain_id_locking(str(domain))\n",
    "    print(baseid, \"id v bazo\")\n",
    "    if(baseid is None):\n",
    "       \n",
    "        robotsstr,sitemap = robots_to_String(url)\n",
    "        \n",
    "        \n",
    "        siteid = update_site_locking(domain,robotsstr,str(sitemap))\n",
    "        \n",
    "        dissaloved,allowed = getAllow_Dissalow(robotsstr)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        robotsstr = get_domain_robots_locking(baseid)\n",
    "        dissaloved,allowed = getAllow_Dissalow(robotsstr)\n",
    "        siteid = baseid\n",
    "        \n",
    "    crawl_site(url, siteid, dissaloved, allowed)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def run(max_threads):\n",
    "    threads = []\n",
    "    for i in range(0, max_threads):\n",
    "        url = frontier.get()\n",
    "        t = Thread(target=pajek, args=(url,))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "        \n",
    "        \n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    print(\"DONE\", len(threads))\n",
    "    threads.clear()\n",
    "    run(4)\n",
    "    \n",
    "reset_db()\n",
    "print('reset')\n",
    "frontier.put(\"https://www.gov.si/\") \n",
    "frontier.put(\"https://evem.gov.si\")\n",
    "frontier.put(\"https://e-uprava.gov.si/\")\n",
    "frontier.put(\"https://www.e-prostor.gov.si/\")\n",
    "run(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65286956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf43d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
