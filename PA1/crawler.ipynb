{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26742cc6",
   "metadata": {},
   "source": [
    "Naloga pajka:\n",
    "1. HTTP downloader and renderer: To retrieve and render a web page.\n",
    "2. Data extractor: Minimal functionalities to extract images and hyperlinks.\n",
    "3. Duplicate detector: To detect already parsed pages.\n",
    "4. URL frontier: A list of URLs waiting to be parsed.\n",
    "5. Datastore: To store the data and additional metadata used by the crawler.\n",
    "\n",
    "TO-DO 2-images exctraction, duplicate detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3846fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "import psycopg2\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def reset_db(conn):\n",
    "    \n",
    "    conn.autocommit = True\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"DELETE FROM crawldb.image\")\n",
    "    cur.execute(\"DELETE FROM crawldb.page_data\")\n",
    "    cur.execute(\"DELETE FROM crawldb.link\")\n",
    "    cur.execute(\"DELETE FROM crawldb.page\")\n",
    "    cur.execute(\"DELETE FROM crawldb.site\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    return\n",
    "  \n",
    "def update_site_locking(domain, sitemap, robotstxt,delay, conn):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"INSERT INTO crawldb.site (domain, robots_content,sitemap_content,delay) VALUES (%s, %s, %s,%s) RETURNING id;\",\n",
    "            (domain, sitemap, robotstxt,delay))\n",
    "            id = -1\n",
    "            if cur.rowcount != 0:\n",
    "                id = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if id != -1:\n",
    "                return id;\n",
    "            else:\n",
    "                print(\"Error with cur in update_site_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_site_locking: \", e)\n",
    "            return 'err'\n",
    "\n",
    "                    \n",
    "    \n",
    "                    \n",
    "def update_page_locking(siteId, url,html_content, status_code, acc_time, page_type_code, conn):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"INSERT INTO crawldb.page (site_id, url,html_content,http_status_code,accessed_time, page_type_code) VALUES (%s,%s,%s,%s,%s,%s) RETURNING id;\",\n",
    "            (siteId, url, html_content,status_code,acc_time,page_type_code))\n",
    "            id = -1\n",
    "            if cur.rowcount != 0:\n",
    "                id = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if id != -1:\n",
    "                return id;\n",
    "            else:\n",
    "                print(\"Error with cur in update_page_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_page_locking: \", e)\n",
    "            return 'err'\n",
    "    \n",
    "                    \n",
    "def update_image_locking(image_data,pageId, conn):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            for img in image_data:\n",
    "                cur.execute(\"INSERT INTO crawldb.image (page_id,filename, content_type,data,accessed_time) VALUES (%s,%s,%s, %s, %s);\", (pageId, img['filename'], img['content_type'],img['data'],img['accessed_time']))\n",
    "                conn.commit()\n",
    "            cur.close()\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_image_locking: \", e)\n",
    "            return 'err'\n",
    "                    \n",
    "    \n",
    "                    \n",
    "def update_page_data_locking(pageId, data_type_code, data, conn):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"INSERT INTO crawldb.page_data (page_id,data_type_code, data) VALUES (%s, %s, %s) RETURNING id;\",\n",
    "            (pageId, data_type_code, data))\n",
    "            id = -1\n",
    "            if cur.rowcount != 0:\n",
    "                id = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if id != -1:\n",
    "                return id;\n",
    "            else:\n",
    "                print(\"Error with cur in update_page_data_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_page_data_locking: \", e)\n",
    "            return 'err'\n",
    "                    \n",
    "    \n",
    "                    \n",
    "                                        \n",
    "def update_link_locking(from_page, to_page, conn):\n",
    "    \n",
    "    with lock:\n",
    "        try:\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"INSERT INTO crawldb.link (from_page,to_page) VALUES (%s, %s);\",\n",
    "            (from_page, to_page))\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_link_locking: \", e)\n",
    "            return 'err'\n",
    "    \n",
    "def get_domain_id_locking(domain, conn):\n",
    "\n",
    "    with lock:\n",
    "        try:\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT id FROM crawldb.site WHERE domain = %s\",\n",
    "            (domain,))\n",
    "            id = None\n",
    "            if cur.rowcount != 0:\n",
    "                id = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if id is not None:\n",
    "                return id;\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(\"Error in get_domain_id_locking: \", e)\n",
    "            return 'err'\n",
    "                \n",
    "                \n",
    "def get_domain_robots_locking(domain, conn):\n",
    "\n",
    "    with lock:\n",
    "        try:\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT robots_content FROM crawldb.site WHERE id = %s\",\n",
    "            (domain,))\n",
    "            robots = -1\n",
    "            if cur.rowcount != 0:\n",
    "                robots = cur.fetchone()[0]\n",
    "\n",
    "            cur.execute(\"SELECT delay FROM crawldb.site WHERE id = %s\",\n",
    "            (domain,))\n",
    "            delay = -1\n",
    "            if cur.rowcount != 0:\n",
    "                delay = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if robots != -1 and delay != -1:\n",
    "                return robots, delay\n",
    "            else:\n",
    "                print(\"Error with cur in update_image_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in get_domain_robots_locking: \", e)\n",
    "            return 'err','err'\n",
    "                 \n",
    "                    \n",
    "def get_last_time_locking(siteid, conn):\n",
    "\n",
    "    with lock:\n",
    "        try:\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"SELECT EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - S.accessed_time)) FROM crawldb.site S WHERE S.id = %s\",\n",
    "            (siteid,))\n",
    "            time = -1\n",
    "            if cur.rowcount != 0:\n",
    "                time = cur.fetchone()[0]\n",
    "\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            if time != -1:\n",
    "                return time\n",
    "            else:\n",
    "                print(\"Error with cur in update_image_locking!\")\n",
    "        except Exception as e:\n",
    "            print(\"Error in get_last_time_locking: \", e)\n",
    "            return 'err'\n",
    "\n",
    "        \n",
    "def update_last_time_locking(siteid, conn):\n",
    "\n",
    "    with lock:\n",
    "        try:\n",
    "            conn.autocommit = True\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"UPDATE crawldb.site SET accessed_time = %s WHERE id = %s; \",\n",
    "            (datetime.datetime.fromtimestamp(time.time()),siteid,))\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_last_time_locking: \", e)\n",
    "            return 'err'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37897c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "ua = 'User-agent'\n",
    "\n",
    "\n",
    "def get_robots_url(url):\n",
    "    domain_url = '{uri.scheme}://{uri.netloc}'.format(uri=urlparse(url))\n",
    "    robots_url = domain_url + '/robots.txt'\n",
    "    return robots_url\n",
    " \n",
    "def read_robots_txt(url):\n",
    "    robot_url = get_robots_url(url)\n",
    "    robot_file = os.popen(f'curl {robot_url}').read()\n",
    "    return robot_file\n",
    " \n",
    "def initialize_dict(url):\n",
    "    robot_file = read_robots_txt(url)\n",
    "    result_data_set = {ua:{}}\n",
    "    for line in robot_file.split(\"\\n\"):\n",
    "        if line.startswith(ua):\n",
    "            result_data_set[ua].update({line.split(':')[1].strip():{}})\n",
    "    keys = []\n",
    "    for key in result_data_set[ua].keys():\n",
    "        keys.append(key)\n",
    "    return result_data_set, keys, robot_file\n",
    "\n",
    "#def make_sitemaps(robots):\n",
    "#    data = []\n",
    "#    lines = str(robots).splitlines()\n",
    "#    for line in lines:\n",
    "#       # print(line)\n",
    "#        if line.startswith('Sitemap:'):\n",
    "#            split = line.split(':', maxsplit=1)\n",
    "#            data.append(split[1].strip())            \n",
    "\n",
    "#    return data\n",
    "def parse_robot(url):\n",
    "    idict = initialize_dict(url)\n",
    "    result_data_set = idict[0]\n",
    "    keys = idict[1]\n",
    "    robot_file = idict[2]\n",
    "    sitemaps=[] #idict[3]\n",
    "    crawl_delay=5\n",
    "    print_flag = False\n",
    "    for i in range(len(keys)):\n",
    "        if i <= len(keys)-2:\n",
    "            end_str = keys[i+1]\n",
    "        else:\n",
    "            end_str = 'We are done'\n",
    " \n",
    "        result_data_set[ua][keys[i]]['Disallow'] = []\n",
    "        result_data_set[ua][keys[i]]['Allow'] = []\n",
    "        for line in robot_file.split(\"\\n\"):\n",
    "            if end_str in line:\n",
    "                print_flag = False\n",
    "            elif keys[i] in line:\n",
    "                print_flag = True\n",
    "            elif print_flag:\n",
    "                if line.startswith('Disallow') or line.startswith('Allow'):\n",
    "                    status = line.split(':')[0].strip()\n",
    "                    val = line.split(':')[1].strip()\n",
    "                    result_data_set[ua][keys[i]][status].append(val)\n",
    "                if line.startswith('Crawl-delay:'):\n",
    "                    split = line.split(':', maxsplit=1)\n",
    "                    crawl_delay=int(split[1].strip())\n",
    "                if line.startswith('Sitemap:'):\n",
    "                    split = line.split(':', maxsplit=1)\n",
    "                    sitemaps.append(split[1].strip())     \n",
    "    #print(\"here\" ,timetoReq)\n",
    "    return result_data_set,sitemaps,crawl_delay\n",
    "\n",
    "def robots_to_String(url):\n",
    "    result_data_set,sitemaps,crawl_delay = parse_robot(url)\n",
    "    ls = {ua:[],'Status':[],'Pattern':[]}\n",
    "    for k,v in result_data_set.items():\n",
    "        for v in result_data_set[k]:\n",
    "            for key,value in result_data_set[k][v].items():\n",
    "                for value in result_data_set[k][v][key]:\n",
    "                    ls[ua].append(v)\n",
    "                    ls['Status'].append(key)\n",
    "                    ls['Pattern'].append(make_absolute(url,value))\n",
    "                    #ls['Pattern'].append(value)\n",
    "    robots_df = pd.DataFrame.from_dict(ls)\n",
    "    return pd.DataFrame.to_string(robots_df),sitemaps,crawl_delay #robots_df\n",
    "\n",
    "def getAllow_Dissalow(data): #rabi pandas DF\n",
    "    data = io.StringIO(data)\n",
    "    df= pd.read_csv(data, sep='\\s+')\n",
    "    if \"User-agent\" in df:\n",
    "        df=df[(df[\"User-agent\"]==\"*\") | (df[\"User-agent\"]==ua)] ##vse ki niso moj uporabnik oz. *#lahko odstranim ker se me ne tičejo\n",
    "        allowed=df[df['Status']==\"Allow\"].Pattern.tolist()\n",
    "        dissaloved=df[df['Status']==\"Disallow\"].Pattern.tolist()\n",
    "    else:\n",
    "        allowed=[]\n",
    "        dissaloved=[]\n",
    "    return dissaloved,allowed\n",
    "\n",
    "data,sitemaps,crawl_delay=robots_to_String(\"https://www.avvo.com/robots.txt\") #klici to da dobis df\n",
    "#print(sitemaps)\n",
    "#print(crawl_delay)\n",
    "dissallowed,allowed=getAllow_Dissalow(data)\n",
    "#print(dissallowed)\n",
    "#print(allowed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c0f50",
   "metadata": {},
   "source": [
    "['https://fortune.com/wp-admin/', 'https://fortune.com/sponsored/', 'https://fortune.com/feeds/', \n",
    " 'https://fortune.com/feed/', 'https://fortune.com/wp-login.php', 'https://fortune.com/wp-signup.php', \n",
    " 'https://fortune.com/press-this.php', 'https://fortune.com/remote-login.php', 'https://fortune.com/activate/',\n",
    " 'https://fortune.com/cgi-bin/', 'https://fortune.com/mshots/v1/', 'https://fortune.com/next/', \n",
    " 'https://fortune.com/sponsored/', 'https://fortune.com/feeds/', 'https://fortune.com/feed/', 'https://fortune.com/wp-login.php', 'https://fortune.com/wp-signup.php', 'https://fortune.com/press-this.php', 'https://fortune.com/remote-login.php', 'https://fortune.com/activate/', 'https://fortune.com/cgi-bin/', 'https://fortune.com/mshots/v1/', 'https://fortune.com/next/']\n",
    "['https://fortune.com/wp-admin/admin-ajax.php']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b688db",
   "metadata": {},
   "source": [
    "V spodnjem delu kode so metode za pridobivanja sitemapov, potrebno je dobiti .xml naslov iz trenutnega linka - kliči get_sitemap(robots.tx), ki iz robots txt pridobi sitemap.xml, spodnje  funkcije se sprehodijo\n",
    "1. poženi urlSitemap=get_site_map(robots.txt)\n",
    "2. get_all_urls_siteMap(urlSitemap)\n",
    "\n",
    "Moj naslov: WEB_DRIVER_LOCATION = \"C:/Work/Magisterij_1_leto/2.semester/ekstrakcijaSplet/Nal1/chromedriver.exe\"\n",
    "Juretov naslov: C:/Users/Pirk/Desktop/faks-mag/ekstrakcija/chromedriver.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a29193",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-d98a32feba33>, line 102)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-d98a32feba33>\"\u001b[1;36m, line \u001b[1;32m102\u001b[0m\n\u001b[1;33m    df = df.append(row, ignore_index=True)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import xmltodict\n",
    "\n",
    "def get_sitemaps(url): #find all sitemaps of sitemap from robot.txt\n",
    "    \"\"\"Scrapes an XML sitemap from the provided URL and returns XML source.\n",
    "    Args:\n",
    "        url (string): Fully qualified URL pointing to XML sitemap.\n",
    "    Returns:\n",
    "        xml (string): XML source of scraped sitemap.\n",
    "    \"\"\"\n",
    "    response = urllib.request.urlopen(url)\n",
    "    xml = BeautifulSoup(response, \n",
    "                         'lxml-xml', \n",
    "                         from_encoding=response.info().get_param('charset'))\n",
    "    return xml\n",
    "\n",
    "def get_sitemap_type(xml):\n",
    "    \"\"\"Parse XML source and returns the type of sitemap.\n",
    "    Args:\n",
    "        xml (string): Source code of XML sitemap.\n",
    "    Returns:\n",
    "        sitemap_type (string): Type of sitemap (sitemap, sitemapindex, or None).\n",
    "    \"\"\"\n",
    "    sitemapindex = xml.find_all('sitemapindex')\n",
    "    sitemap = xml.find_all('urlset')\n",
    "    #print(sitemap)\n",
    "    if sitemapindex:\n",
    "        return 'sitemapindex' #vsebujejo linke na otroke\n",
    "    elif sitemap:\n",
    "        return 'urlset' #direktni linki\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "def get_child_sitemaps(xml):\n",
    "    \"\"\"Return a list of child sitemaps present in a XML sitemap file.\n",
    "    Args:\n",
    "        xml (string): XML source of sitemap. \n",
    "    Returns:\n",
    "        sitemaps (list): Python list of XML sitemap URLs.\n",
    "    \"\"\"\n",
    "    sitemaps = xml.find_all(\"sitemap\")\n",
    "    output = []\n",
    "    for sitemap in sitemaps:\n",
    "        output.append(sitemap.findNext(\"loc\").text)\n",
    "  \n",
    "    return output\n",
    "def sitemap_to_dataframe(xml, name=None, data=None, verbose=False):\n",
    "    \"\"\"Read an XML sitemap into a Pandas dataframe. \n",
    "\n",
    "    Args:\n",
    "        xml (string): XML source of sitemap. \n",
    "        name (optional): Optional name for sitemap parsed.\n",
    "        verbose (boolean, optional): Set to True to monitor progress.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Pandas dataframe of XML sitemap content. \n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns=['loc', 'changefreq', 'priority', 'domain', 'sitemap_name'])\n",
    "\n",
    "    urls = xml.find_all(\"url\")\n",
    "  \n",
    "    for url in urls:\n",
    "\n",
    "        if xml.find(\"loc\"):\n",
    "            loc = url.findNext(\"loc\").text\n",
    "            parsed_uri = urlparse(loc)\n",
    "            domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        else:\n",
    "            loc = ''\n",
    "            domain = ''\n",
    "\n",
    "        if xml.find(\"changefreq\"):\n",
    "            changefreq = url.findNext(\"changefreq\").text\n",
    "        else:\n",
    "            changefreq = ''\n",
    "\n",
    "        if xml.find(\"priority\"):\n",
    "            priority = url.findNext(\"priority\").text\n",
    "        else:\n",
    "            priority = ''\n",
    "\n",
    "        if name:\n",
    "            sitemap_name = name\n",
    "        else:\n",
    "            sitemap_name = ''\n",
    "              \n",
    "        row = {\n",
    "            'domain': domain,\n",
    "            'loc': loc,\n",
    "            'changefreq': changefreq,\n",
    "            'priority': priority,\n",
    "            'sitemap_name': sitemap_name,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            #print(row)\n",
    "\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    return df\n",
    "def get_all_urls_SiteMap(url): #provide xml of a site , from robot.txt ,... \n",
    "    \"\"\"Return a dataframe containing all of the URLs from a site's XML sitemaps.\n",
    "    Args:\n",
    "        url (string): URL of site's XML sitemap. Usually located at /sitemap.xml\n",
    "    Returns:\n",
    "        df (dataframe): Pandas dataframe containing all sitemap content. \n",
    "\n",
    "    \"\"\"  \n",
    "    xml = get_sitemaps(url)\n",
    "    sitemap_type = get_sitemap_type(xml)\n",
    "    if sitemap_type =='sitemapindex':\n",
    "        sitemaps = get_child_sitemaps(xml)\n",
    "    else:\n",
    "        sitemaps = [url]\n",
    "    df = pd.DataFrame(columns=['loc', 'changefreq', 'priority', 'domain', 'sitemap_name'])\n",
    "    for sitemap in sitemaps:\n",
    "        #print(sitemap)\n",
    "        sitemap_xml = get_sitemaps(sitemap) # ce želimo imeti vse povezave sitemapov\n",
    "        #  sitemaps_all.append(sitemap_xml) \n",
    "        df_sitemap = sitemap_to_dataframe(sitemap_xml, name=sitemap)\n",
    "        #print(sitemap_xml)\n",
    "        #print(df_sitemap)\n",
    "        #print(\"end\")\n",
    "        df = pd.concat([df, df_sitemap], ignore_index=True)\n",
    "        #print(sitemap)\n",
    "        #file = urllib.request.urlopen(sitemap)  #odpri xml in pridobi podatke\n",
    "        #data = file.read()\n",
    "        #file.close()\n",
    "        #data = xmltodict.parse(data)\n",
    "        #print(data)\n",
    "    return df\n",
    "\n",
    "url=\"https://www.gov.si/sitemap.xml\" #dobimo ga recimo z robots.txt\n",
    "dataFrame = get_all_urls_SiteMap(url)\n",
    "#print(dataFrame.head())\n",
    "#print(dataFrame.sitemap_name.value_counts())\n",
    "xml=get_sitemap(url)\n",
    "#sitemap_type = get_sitemap_type(xml)\n",
    "#if(sitemap_type==\"sitemapindex\"): #če je \n",
    "    #child_sitemaps = get_child_sitemaps(xml)\n",
    "    #print(child_sitemaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43993a62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRONTIER:  ['https://www.gov.si/', 'https://evem.gov.si', 'https://e-uprava.gov.si/', 'https://www.e-prostor.gov.si/']\n",
      "DOMAINS:  {}\n",
      "VISITED:  {}\n",
      "reset\n",
      "IN INIT\n",
      "MAX THREADS:  4\n",
      "IN RUN\n",
      "https://www.gov.si/ from thread 0\n",
      "https://e-uprava.gov.si/ from thread 1\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "ni nastavljen last time\n",
      "ni nastavljen last time\n",
      "https://www.gov.si/teme/koronavirus-sars-cov-2/mobilna-aplikacija-ostanizdrav/ from thread 2\n",
      "https://www.e-prostor.gov.si/ from thread 3\n",
      "2116 id v bazo\n",
      "2115 id v bazo\n",
      "\n",
      "\n",
      "3607.837207 AAAAAAAAAAAAAAAAAA\n",
      "IN RUN\n",
      "https://www.e-prostor.gov.si/o-portalu/ from thread 0\n",
      "https://evem.gov.si/sl/ from thread 1\n",
      "https://evem.gov.si/sl/ from thread 2\n",
      "https://www.e-prostor.gov.si/kontakt/ from thread 3\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "\n",
      "\n",
      "3609.071723 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3609.11795 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3609.163048 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3609.209862 AAAAAAAAAAAAAAAAAA\n",
      "Error in update_page_locking:  duplicate key value violates unique constraint \"unq_url_idx\"\n",
      "DETAIL:  Key (url)=(https://www.e-prostor.gov.si/access-to-geodetic-data/ordering-data/) already exists.\n",
      "\n",
      "IN RUN\n",
      "https://evem.gov.si/en/ from thread 0\n",
      "https://www.e-prostor.gov.si/dostop-do-podatkov/dostop-do-podatkov/ from thread 1\n",
      "https://www.e-prostor.gov.si/metapodatki/ from thread 2\n",
      "https://evem.gov.si/evem/sicas/uporabnik/prijava.evem from thread 3\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "2115 id v bazo\n",
      "2116 id v bazo\n",
      "\n",
      "\n",
      "3610.39969 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3611.534618 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3610.489748 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3610.534503 AAAAAAAAAAAAAAAAAA\n",
      "IN RUN\n",
      "https://www.e-prostor.gov.si/izobrazevanja/izobrazevanje-geodetov-po-zgeod-1/ from thread 0\n",
      "https://evem.gov.si/evem/cert/uporabnik/prijava.evem from thread 1\n",
      "https://www.e-prostor.gov.si/fileadmin/DPKS/Transformacije_ETRS89/Aplikacije/ETRS89-SI.zip from thread 2\n",
      "https://evem.gov.si/sl/portal-in-tocke-spot/o-portalu-spot/#e23671 from thread 3\n",
      "2115 id v bazo\n",
      "2116 id v bazo\n",
      "2115 id v bazo\n",
      "2116 id v bazo\n",
      "\n",
      "\n",
      "3608.026566 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3608.080534 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3608.226593 AAAAAAAAAAAAAAAAAA\n",
      "IN RUN\n",
      "https://evem.gov.si/sl/poslovanje/ from thread 0\n",
      "https://www.mvn.e-prostor.gov.si/evidence/evidenca-trga-nepremicnin/porocanje-v-etn/ from thread 1\n",
      "https://evem.gov.si/sl/portal-in-tocke-spot/ from thread 2\n",
      "https://evem.gov.si/sl/e-postopki-in-storitve/ from thread 3\n",
      "None id v bazo\n",
      "2115 id v bazo\n",
      "None id v bazo\n",
      "2115 id v bazo\n",
      "\n",
      "\n",
      "3607.43606 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3607.488024 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "ni nastavljen last time\n",
      "ni nastavljen last time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 710, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1040, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connection.py\", line 426, in connect\n",
      "    tls_in_tls=tls_in_tls,\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 450, in ssl_wrap_socket\n",
      "    sock, context, tls_in_tls, server_hostname=server_hostname\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 407, in wrap_socket\n",
      "    _context=self, _session=session)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 817, in __init__\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 1077, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 689, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 450, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 786, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='vprasalnik.gu.gov.si', port=443): Max retries exceeded with url: /DAZK/faces/Login.jspx (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)'),))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 276, in pajek\n",
      "    crawl_site(url, siteid, dissaloved, allowed, delay , conn)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 151, in crawl_site\n",
      "    response = requests.get(WEB_PAGE_ADDRESS)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 517, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='vprasalnik.gu.gov.si', port=443): Max retries exceeded with url: /DAZK/faces/Login.jspx (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)'),))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN RUN\n",
      "https://evem.gov.si/evem/cert/uporabnik/prijava.evem from thread 0\n",
      "https://evem.gov.si/evem/cert/uporabnik/prijava.evem from thread 1\n",
      "http://prostor3.gov.si/ETN-JV/ from thread 2\n",
      "https://evem.gov.si/evem/cert/uporabnik/prijava.evem from thread 3\n",
      "None id v bazo\n",
      "2115 id v bazo\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "\n",
      "\n",
      "3612.391176 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "ni nastavljen last time\n",
      "ni nastavljen last time\n",
      "ni nastavljen last time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connection.py\", line 175, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\connection.py\", line 72, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\socket.py\", line 745, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 710, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 398, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connection.py\", line 239, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\http\\client.py\", line 1287, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\http\\client.py\", line 1333, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\http\\client.py\", line 1282, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\http\\client.py\", line 1042, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\http\\client.py\", line 980, in send\n",
      "    self.connect()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connection.py\", line 205, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connection.py\", line 187, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000002B3F2E5B2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 450, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 786, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='prostor3.sigov.si', port=80): Max retries exceeded with url: /preg/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002B3F2E5B2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 276, in pajek\n",
      "    crawl_site(url, siteid, dissaloved, allowed, delay , conn)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 151, in crawl_site\n",
      "    response = requests.get(WEB_PAGE_ADDRESS)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='prostor3.sigov.si', port=80): Max retries exceeded with url: /preg/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002B3F2E5B2B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))\n",
      "\n",
      "Exception in thread Thread-28:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 710, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1040, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connection.py\", line 426, in connect\n",
      "    tls_in_tls=tls_in_tls,\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 450, in ssl_wrap_socket\n",
      "    sock, context, tls_in_tls, server_hostname=server_hostname\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 407, in wrap_socket\n",
      "    _context=self, _session=session)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 817, in __init__\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 1077, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 689, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLError: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:852)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 450, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 786, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='prostor-s.gov.si', port=443): Max retries exceeded with url: /preg/ (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:852)'),))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 276, in pajek\n",
      "    crawl_site(url, siteid, dissaloved, allowed, delay , conn)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 151, in crawl_site\n",
      "    response = requests.get(WEB_PAGE_ADDRESS)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 517, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='prostor-s.gov.si', port=443): Max retries exceeded with url: /preg/ (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:852)'),))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN RUN\n",
      "https://eprostor.gov.si/osebni/ from thread 0\n",
      "https://evem.gov.si/evem/cert/uporabnik/prijava.evem from thread 1\n",
      "https://evem.gov.si/sl/poslovanje/zaposlovanje-in-delovno-razmerje/napotitev-delavca-na-delo-na-domu/ from thread 2\n",
      "https://eprostor.sigov.si/pgp/index.jsp from thread 3\n",
      "2115 id v bazo\n",
      "None id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "\n",
      "\n",
      "3612.370373 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3612.412379 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3612.496416 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-32:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 710, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1040, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connection.py\", line 426, in connect\n",
      "    tls_in_tls=tls_in_tls,\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 450, in ssl_wrap_socket\n",
      "    sock, context, tls_in_tls, server_hostname=server_hostname\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 407, in wrap_socket\n",
      "    _context=self, _session=session)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 817, in __init__\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 1077, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 689, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLError: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:852)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 450, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 786, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='sicas-x509si.gov.si', port=443): Max retries exceeded with url: /idpX509/login?policy=KDP-SI&service=https%3A%2F%2Fsicas.gov.si%2Fbl%2FhandleIdpResponse%3FrelayState%3Dca703fc2-2840-4100-9b75-ab4e5250012b&lang=si&spName=SPOT (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:852)'),))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 276, in pajek\n",
      "    crawl_site(url, siteid, dissaloved, allowed, delay , conn)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 151, in crawl_site\n",
      "    response = requests.get(WEB_PAGE_ADDRESS)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 667, in send\n",
      "    history = [resp for resp in gen]\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 667, in <listcomp>\n",
      "    history = [resp for resp in gen]\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 245, in resolve_redirects\n",
      "    **adapter_kwargs\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 517, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='sicas-x509si.gov.si', port=443): Max retries exceeded with url: /idpX509/login?policy=KDP-SI&service=https%3A%2F%2Fsicas.gov.si%2Fbl%2FhandleIdpResponse%3FrelayState%3Dca703fc2-2840-4100-9b75-ab4e5250012b&lang=si&spName=SPOT (Caused by SSLError(SSLError(1, '[SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:852)'),))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ni nastavljen last time\n",
      "IN RUN\n",
      "https://gis.gov.si/ezkn/ from thread 0\n",
      "https://www.e-prostor.gov.si/fileadmin/DPKS/Transformacija_v_novi_KS/Aplikacije/3tra.zip from thread 1\n",
      "https://www.e-prostor.gov.si/fileadmin/DPKS/Transformacije_ETRS89/Aplikacije/ETRS89-SI.zip from thread 2\n",
      "https://www.e-prostor.gov.si/fileadmin/DPKS/Transformacije_ETRS89/Aplikacije/ITRS-SI.zip from thread 3\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-31:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 710, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 386, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 1040, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connection.py\", line 426, in connect\n",
      "    tls_in_tls=tls_in_tls,\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 450, in ssl_wrap_socket\n",
      "    sock, context, tls_in_tls, server_hostname=server_hostname\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 493, in _ssl_wrap_socket_impl\n",
      "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 407, in wrap_socket\n",
      "    _context=self, _session=session)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 817, in __init__\n",
      "    self.do_handshake()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 1077, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\ssl.py\", line 689, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 450, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\connectionpool.py\", line 786, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='egp.gu.gov.si', port=443): Max retries exceeded with url: /egp/ (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)'),))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 276, in pajek\n",
      "    crawl_site(url, siteid, dissaloved, allowed, delay , conn)\n",
      "  File \"<ipython-input-16-3b9873a90f7b>\", line 151, in crawl_site\n",
      "    response = requests.get(WEB_PAGE_ADDRESS)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\miham\\anaconda3\\envs\\wier\\lib\\site-packages\\requests\\adapters.py\", line 517, in send\n",
      "    raise SSLError(e, request=request)\n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='egp.gu.gov.si', port=443): Max retries exceeded with url: /egp/ (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)'),))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2115 id v bazo\n",
      "\n",
      "\n",
      "3605.463646 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3605.50747 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3605.553345 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3605.594116 AAAAAAAAAAAAAAAAAA\n",
      "IN RUN\n",
      "https://vprasalnik.gu.gov.si/DAZK/faces/Login.jspx from thread 0\n",
      "https://www.mvn.e-prostor.gov.si/evidence/evidenca-trga-nepremicnin/porocanje-v-etn/ from thread 1\n",
      "https://eprostor.gov.si/javni/ from thread 2\n",
      "http://prostor3.gov.si/javni-arhiv/login.jsp?jezik=sl from thread 3\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "\n",
      "\n",
      "3600.642293 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.687698 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.731553 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.779008 AAAAAAAAAAAAAAAAAA\n",
      "IN RUN\n",
      "http://prostor3.gov.si/ETN-JV/ from thread 0\n",
      "https://prostor-s.gov.si/preg/ from thread 1\n",
      "http://prostor3.sigov.si/preg/ from thread 2\n",
      "https://eprostor.gov.si/osebni/ from thread 3\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "\n",
      "\n",
      "3600.646236 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.687726 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.730347 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.770181 AAAAAAAAAAAAAAAAAA\n",
      "IN RUN\n",
      "https://egp.gu.gov.si/egp/ from thread 0\n",
      "https://eprostor.sigov.si/pgp/index.jsp from thread 1\n",
      "https://gis.gov.si/ezkn/ from thread 2\n",
      "https://www.e-prostor.gov.si/fileadmin/DPKS/Transformacija_v_novi_KS/Aplikacije/3tra.zip from thread 3\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "\n",
      "\n",
      "3600.644445 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.695273 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.739286 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.779281 AAAAAAAAAAAAAAAAAA\n",
      "IN RUN\n",
      "https://www.e-prostor.gov.si/aplikacije/ from thread 0\n",
      "https://www.e-prostor.gov.si/zbirke-prostorskih-podatkov/drzavni-prostorski-koordinatni-sistem/horizontalna-sestavina/stalne-postaje-drzavnega-omrezja-gnss-signal/#tab2-1609 from thread 1\n",
      "https://www.e-prostor.gov.si/aktualno/obvestilo-uporabnikom-o-nacrtovanih-spremembah-v-is-kataster-nepremicnin/ from thread 2\n",
      "https://www.e-prostor.gov.si/aktualno/strukture-in-testni-podatki-katastra-nepremicnin/ from thread 3\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "\n",
      "\n",
      "3600.64281 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.688379 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.742863 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3599.97826 AAAAAAAAAAAAAAAAAA\n",
      "IN RUN\n",
      "https://www.e-prostor.gov.si/zbirke-prostorskih-podatkov/drzavni-prostorski-koordinatni-sistem/horizontalna-sestavina/stalne-postaje-drzavnega-omrezja-gnss-signal/#tab2-1609 from thread 0\n",
      "https://www.e-prostor.gov.si/aktualno/obvestilo-uporabnikom-o-nacrtovanih-spremembah-v-is-kataster-nepremicnin/ from thread 1\n",
      "https://www.e-prostor.gov.si/aktualno/strukture-in-testni-podatki-katastra-nepremicnin/ from thread 2\n",
      "https://www.e-prostor.gov.si/zbirke-prostorskih-podatkov/drzavni-prostorski-koordinatni-sistem/horizontalna-sestavina/stalne-postaje-drzavnega-omrezja-gnss-signal/#tab2-1609 from thread 3\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "None id v bazo\n",
      "2115 id v bazo\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3600.518393 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "ni nastavljen last time\n",
      "ni nastavljen last time\n",
      "ni nastavljen last time\n",
      "IN RUN\n",
      "https://www.e-prostor.gov.si/aktualno-arhiv/ from thread 0\n",
      "https://www.e-prostor.gov.si/fileadmin/Aktualne_povezave/00_DPK250_2019_admin_raster.jpg from thread 1\n",
      "https://evem.gov.si/sl/politika-zasebnosti/ from thread 2\n",
      "https://evem.gov.si/sl/teme/dostopnost/ from thread 3\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "2115 id v bazo\n",
      "None id v bazo\n",
      "\n",
      "\n",
      "3619.626601 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3619.6697 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3619.714823 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "ni nastavljen last time\n",
      "IN RUN\n",
      "https://www.mvn.e-prostor.gov.si from thread 0\n",
      "https://www.projekt.e-prostor.gov.si/ from thread 1\n",
      "https://www.e-prostor.gov.si/zbirke-prostorskih-podatkov/topografski-in-kartografski-podatki/aerofotografije/ from thread 2\n",
      "https://www.e-prostor.gov.si/zbirke-prostorskih-podatkov/drzavni-prostorski-koordinatni-sistem/ from thread 3\n",
      "None id v bazo\n",
      "2122 id v bazo\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "\n",
      "\n",
      "3676.883455 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3676.965412 AAAAAAAAAAAAAAAAAA\n",
      "ni nastavljen last time\n",
      "IN RUN\n",
      "https://www.mvn.e-prostor.gov.si/evidence/evidenca-trga-nepremicnin/ from thread 0\n",
      "https://www.e-prostor.gov.si/zbirke-prostorskih-podatkov/mnozicno-vrednotenje-nepremicnin/ from thread 1\n",
      "https://www.e-prostor.gov.si/zbirke-prostorskih-podatkov/nepremicnine/register-nepremicnin/ from thread 2\n",
      "https://www.e-prostor.gov.si/zbirke-prostorskih-podatkov/topografski-in-kartografski-podatki/register-zemljepisnih-imen/ from thread 3\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "\n",
      "\n",
      "3620.307217 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3620.350968 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3620.392906 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3620.43103 AAAAAAAAAAAAAAAAAA\n",
      "IN RUN\n",
      "https://www.e-prostor.gov.si/zbirke-prostorskih-podatkov/zbirni-kataster-gospodarske-javne-infrastrukture/ from thread 0\n",
      "https://www.e-prostor.gov.si/dostop-do-podatkov/dostop-do-podatkov/ from thread 1\n",
      "https://www.e-prostor.gov.si/dostop-do-podatkov/dostop-do-podatkov/mapa/vzorci-podatkov/ from thread 2\n",
      "https://www.e-prostor.gov.si/informacije/vsa-pogosta-vprasanja/?no_cache=1 from thread 3\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "None id v bazo\n",
      "\n",
      "\n",
      "3609.367104 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3609.413674 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "None AAAAAAAAAAAAAAAAAA\n",
      "ni nastavljen last time\n",
      "IN RUN\n",
      "https://www.mvn.e-prostor.gov.si/ from thread 0\n",
      "http://e-uprava.gov.si/ from thread 1\n",
      "http://www.geoportal.gov.si/ from thread 2\n",
      "https://www.e-prostor.gov.si/varstvo-osebnih-podatkov-in-uporaba-informacij-javnega-znacaja/ from thread 3\n",
      "2127 id v bazo\n",
      "None id v bazo\n",
      "2116 id v bazo\n",
      "2116 id v bazo\n",
      "\n",
      "\n",
      "3614.623593 AAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "3614.671518 AAAAAAAAAAAAAAAAAA\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import queue\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "from url_normalize import url_normalize\n",
    "import hashlib\n",
    "import datetime\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "import json\n",
    "import schedule\n",
    "#import magic\n",
    "\n",
    "\n",
    "visited_urls = []\n",
    "visited = {}\n",
    "domains = {}\n",
    "frontier = queue.Queue()\n",
    "#WEB_DRIVER_LOCATION = \"C:/Users/Pirk/Desktop/faks-mag/ekstrakcija/chromedriver.exe\" #jure\n",
    "#WEB_DRIVER_LOCATION = \"C:/Work/Magisterij_1_leto/2.semester/ekstrakcijaSplet/Nal1/chromedriver.exe\" #matjaž\n",
    "WEB_DRIVER_LOCATION = \"C:/Users/miham/Documents/Faks/IEPS/chromedriver.exe\" #Miha\n",
    "FRONTIER_LOCATION = \"C:/Users/miham/Documents/GitHub/IESP-1/PA1/frontier.txt\" #Miha\n",
    "DOMAIN_LOCATION = \"C:/Users/miham/Documents/GitHub/IESP-1/PA1/domains.txt\" #Miha\n",
    "VISITED_LOCATION = \"C:/Users/miham/Documents/GitHub/IESP-1/PA1/visited.txt\" #Miha\n",
    "TIMEOUT = 5\n",
    "#url =  \"https://www.gov.si/\"\n",
    "sha256 = hashlib.sha256()\n",
    "\n",
    "def is_absolute(link):\n",
    "    return bool(urlparse(link).netloc)\n",
    "\n",
    "def is_link(link):\n",
    "    if (len(re.findall('.:\\/\\/.',link))>0):\n",
    "        return True\n",
    "    return False\n",
    "    #logika - preveri če je stvar valid link ki ga damo v frontier\n",
    "    \n",
    "def make_absolute(baselink,link):\n",
    "    \n",
    "    if (True != is_absolute(link)):\n",
    "        return(urljoin(baselink,link))\n",
    "\n",
    "    elif(link[0] == '/' and link[1] == '/'):\n",
    "        return 'https:' + link\n",
    "\n",
    "    return link\n",
    "    #logika - prepozna če je link relativen inga spremeni v absolutnega\n",
    "\n",
    "def get_robotstxt(link):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"user-agent=fri-ieps-TEST\")\n",
    "    chrome_options.add_argument('headless')\n",
    "    driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "    driver.get(link + '/robots.txt')\n",
    "    time.sleep(TIMEOUT)\n",
    "    html = driver.page_source\n",
    "    bsObj = BeautifulSoup(html, 'html.parser')\n",
    "    robots = bsObj.find('pre').contents\n",
    "    return robots\n",
    "\n",
    "def get_sitemap(robotstxt):\n",
    "    robotarray = robotstxt.split()\n",
    "    if(\"Sitemap:\" in robotarray):\n",
    "        index = robotarray.index(\"Sitemap:\")\n",
    "        return robotarray[index+1]\n",
    "    return None\n",
    "\n",
    "def getsitemapContext(url):\n",
    "    site = url.find_all(\"sitemap\")\n",
    "\n",
    "def checkduplicate(html):\n",
    "    #print(hashlib.sha224(str(html).encode(\"utf-8\")).hexdigest())\n",
    "    \n",
    "    if (hashlib.sha224(str(html).encode(\"utf-8\")).hexdigest() in visited):\n",
    "        return visited[hashlib.sha224(str(html).encode(\"utf-8\")).hexdigest()]\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def get_ending(url):\n",
    "    stays_same = ['pdf', 'doc', 'docx', 'ppt', 'pptx']\n",
    "    \n",
    "    split_url = url.split('.')\n",
    "    if(len(split_url)>0):\n",
    "        last = split_url[-1]\n",
    "        if last in stays_same:\n",
    "            return last\n",
    "        else:\n",
    "            return 'html'\n",
    "    else:\n",
    "        return 'drop'\n",
    "\n",
    "    \n",
    "def isallowed(url, dissaloved):\n",
    "\n",
    "    if(url == None):\n",
    "        return False\n",
    "    \n",
    "    if('gov.si' not in url):\n",
    "        return False\n",
    "    \n",
    "    if('mailto' in url):\n",
    "        return False\n",
    "    \n",
    "    if (len(re.findall('^.*tel:\\d{1,9}.*$',url))>0):\n",
    "        return False\n",
    "    \n",
    "    for link in dissaloved:\n",
    "        if(link == url):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def crawl_site(url,siteid,dissaloved, allowed, delay, conn):\n",
    "\n",
    "    if(delay is None):\n",
    "        TIMEOUT = 5\n",
    "    elif(delay == -1):\n",
    "        TIMEOUT = 5\n",
    "    else:\n",
    "        TIMEOUT = delay\n",
    "            \n",
    "    if url not in visited_urls:\n",
    "        with lock:\n",
    "            visited_urls.append(url)\n",
    "\n",
    "        pagetype = requests.head(url).headers['content-type']\n",
    "        if('html' not in pagetype):\n",
    "            return\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        time_diff = get_last_time_locking(siteid,conn);\n",
    "        print('\\n')\n",
    "        print(time_diff,'AAAAAAAAAAAAAAAAAA')\n",
    "        if(time_diff is None):\n",
    "            time.sleep(5)\n",
    "            print(\"ni nastavljen last time\")\n",
    "        \n",
    "        elif(int(time_diff) < int(TIMEOUT)):\n",
    "            print(int(TIMEOUT)-int(time_diff),\"spim toliko casa\")\n",
    "            time.sleep(int(TIMEOUT)-int(time_diff))\n",
    "        \n",
    "            \n",
    "        WEB_PAGE_ADDRESS = url\n",
    "        baseurl = re.match('^.+?[^\\/:](?=[?\\/]|$)', url).group(0)\n",
    "        #print(baseurl,'baseurl')\n",
    "        acc_time = datetime.datetime.fromtimestamp(time.time())\n",
    "\n",
    "        response = requests.get(WEB_PAGE_ADDRESS)\n",
    "        statusCode = response.status_code\n",
    "        update_last_time_locking(siteid,conn)\n",
    "\n",
    "        \n",
    "        if(statusCode >= 300):\n",
    "            update_page_locking(siteid, url_normalize(url),'', statusCode, acc_time, 'HTML', conn)\n",
    "            return\n",
    "    \n",
    "             \n",
    "        \n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"user-agent=fri-ieps-man_shrugging\")\n",
    "        chrome_options.add_argument('headless')\n",
    "        driver = webdriver.Chrome(WEB_DRIVER_LOCATION, options=chrome_options)\n",
    "        try:\n",
    "            driver.get(WEB_PAGE_ADDRESS)\n",
    "        except Exception as e:\n",
    "            print(\"Error in update_link_locking: \", e)\n",
    "            return\n",
    "            \n",
    "\n",
    "\n",
    "        time.sleep(TIMEOUT)\n",
    "        \n",
    "\n",
    "\n",
    "        html = driver.page_source       \n",
    "        isVisited = checkduplicate(html)\n",
    "\n",
    "        if(isVisited != -1):\n",
    "            pageid = update_page_locking(siteid, url_normalize(url),'', statusCode, acc_time, 'DUPLICATE', conn)\n",
    "            if(pageid == 'err'):\n",
    "                return\n",
    "            update_link_locking(isVisited, pageid, conn)\n",
    "            return\n",
    "\n",
    "        \n",
    "        bsObj = BeautifulSoup(html, 'html.parser')\n",
    "        links = bsObj.find_all('a', href=True)\n",
    "        img_tags = bsObj.findAll('img')\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        page_type_code = \"\"\n",
    "        #print(url_normalize(url))\n",
    "        head = bsObj.find('head').contents\n",
    "        if head is None:\n",
    "            page_type_code = \"BINARY\"\n",
    "        else:\n",
    "            page_type_code = \"HTML\"\n",
    "            \n",
    "        \n",
    "        pageid = update_page_locking(siteid, url_normalize(url),str(html), str(statusCode), acc_time, page_type_code, conn)\n",
    "        if(pageid == 'err'):\n",
    "            return\n",
    "        \n",
    "        \n",
    "        with lock:\n",
    "            visited[hashlib.sha224(str(html).encode(\"utf-8\")).hexdigest()] = pageid\n",
    "            \n",
    "        img_urls = [img['src'] for img in img_tags]\n",
    "        img_db_objs = []    \n",
    "        for img_url in img_urls:\n",
    "            split_url = img_url.split('/')\n",
    "            name = split_url[-1]\n",
    "            filetype = name[-4:]\n",
    "            name = name[:-4]\n",
    "            data = img_url\n",
    "            accessed_time = time.time()\n",
    "\n",
    "            img_db = {\n",
    "                'filename': name,\n",
    "                'content_type': filetype,\n",
    "                'data': data,\n",
    "                'accessed_time': acc_time\n",
    "            }\n",
    "\n",
    "            img_db_objs.append(img_db)\n",
    "\n",
    "        update_image_locking(img_db_objs,pageid, conn)\n",
    "            \n",
    "        for link in links:\n",
    "            if (len(link.attrs['href'])>0 and link.attrs['href'][0] != '#'):\n",
    "                absolute_link = make_absolute(baseurl,link.attrs['href'])\n",
    "                ending = get_ending(absolute_link)\n",
    "                if(ending == 'html'):\n",
    "                    if(isallowed(absolute_link,dissaloved)):\n",
    "                        with lock:\n",
    "                            frontier.put(absolute_link)\n",
    "                elif(ending == 'drop'):\n",
    "                    continue\n",
    "                else: \n",
    "                    #print(\"TU SE NEKE ZAPISE V BAZO\")\n",
    "                    update_page_data_locking(pageid, ending.upper(), '',conn)\n",
    "                    #zapisi v bazo\n",
    "        \n",
    "       \n",
    "        \n",
    "def pajek(url, conn):\n",
    "    \n",
    "    url = frontier.get()\n",
    "    domain = re.match('^.+?[^\\/:](?=[?\\/]|$)', url).group(0)\n",
    "    delay = -1\n",
    "    siteid = '666'\n",
    "    baseid = get_domain_id_locking(str(domain), conn)\n",
    "    print(baseid, \"id v bazo\")\n",
    "    if(baseid is None):\n",
    "       \n",
    "        robotsstr,sitemap,delay = robots_to_String(url)\n",
    "        \n",
    "        \n",
    "        siteid = update_site_locking(domain,robotsstr,str(sitemap),delay, conn)\n",
    "        \n",
    "        dissaloved,allowed = getAllow_Dissalow(robotsstr)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        robotsstr,delay = get_domain_robots_locking(baseid, conn)\n",
    "        dissaloved,allowed = getAllow_Dissalow(robotsstr)\n",
    "        siteid = baseid\n",
    "      \n",
    "    crawl_site(url, siteid, dissaloved, allowed, delay , conn)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def run(max_threads, conn):\n",
    "    threads = []\n",
    "    print(\"IN RUN\")\n",
    "    if conn.closed == 1:\n",
    "        conn = psycopg2.connect(host=\"localhost\",dbname='postgres', user=\"postgres\", password=\"admin\")\n",
    "\n",
    "    for i in range(0, max_threads):\n",
    "        url = frontier.get()\n",
    "        print(url, \"from thread\", i)\n",
    "        t = Thread(target=pajek, args=(url, conn,))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "        \n",
    "        \n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    #print(\"DONE\", len(threads))\n",
    "    threads.clear()\n",
    "    return\n",
    "    \n",
    "def init(conn):\n",
    "    print(\"IN INIT\")\n",
    "    #max_threads = 20;\n",
    "    while True:\n",
    "        max_threads = 20;\n",
    "        schedule.run_pending() #checks if it should run any scheduled tasks\n",
    "        if(frontier.qsize()<max_threads):\n",
    "            max_threads = frontier.qsize()\n",
    "            print(\"MAX THREADS: \", max_threads)\n",
    "            if(max_threads==0):\n",
    "                print('PRAZNA VRSTA')\n",
    "        try:\n",
    "            run(max_threads, conn)\n",
    "        except e:\n",
    "            print(\"ERROR IN INIT: \", e)\n",
    "    return\n",
    "    \n",
    "\n",
    "def save_data():\n",
    "    print(\"SAVING...\")\n",
    "    frontier_list = list(frontier.queue)\n",
    "    with open(FRONTIER_LOCATION, 'w') as f:\n",
    "        for url in frontier_list:\n",
    "            f.write('%s\\n' % url)\n",
    "        f.close()\n",
    "\n",
    "    with open(DOMAIN_LOCATION, 'w') as d:\n",
    "        d.write(json.dumps(domains))\n",
    "        d.close()\n",
    "\n",
    "    with open(VISITED_LOCATION, 'w') as v:\n",
    "        v.write(json.dumps(domains))\n",
    "        v.close()\n",
    "\n",
    "schedule.every().hour.do(save_data) #schedules to run save_data every hour\n",
    "\n",
    "#check for frontier.txt, load it into frontier queue\n",
    "with open(FRONTIER_LOCATION, 'r') as f:\n",
    "    with frontier.mutex:\n",
    "        frontier.queue.clear()  #ce je slucajno ostalo kaj v frontierju, za vsak slucaj\n",
    "        urls = f.readlines()\n",
    "    for url in urls:\n",
    "        frontier.put(url.strip()) #ponovno dodajanje\n",
    "    f.close()\n",
    "\n",
    "#loads domains and visited dictionaries\n",
    "with open(DOMAIN_LOCATION, 'r') as d:\n",
    "    data = d.read()\n",
    "    domains = json.loads(data)\n",
    "    d.close()\n",
    "\n",
    "with open(VISITED_LOCATION, 'r') as v:\n",
    "    data = v.read()\n",
    "    visited = json.loads(data)\n",
    "    v.close()\n",
    "\n",
    "frontier_list = list(frontier.queue)\n",
    "\n",
    "print(\"FRONTIER: \", frontier_list)\n",
    "print(\"DOMAINS: \", domains)\n",
    "print(\"VISITED: \", visited)\n",
    "\n",
    "conn = psycopg2.connect(host=\"83.212.127.54\",dbname='crawler', user=\"test\", password=\"fricrawl\")\n",
    "#reset_db(conn)\n",
    "#print('reset')\n",
    "init(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65286956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf43d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e304e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
